ğŸ“š Literature Summarizer â€” AI-Powered PDF Analysis App

A production-ready, local AI application that enables users to upload academic or technical PDFs, generate concise summaries, and ask semantic questions over document content.

Built with LangChain, FAISS, Ollama, and Streamlit, this project demonstrates end-to-end LLM application development â€” from document ingestion and embeddings to retrieval-augmented generation (RAG) and UI deployment.



ğŸš€ Project Highlights

- End-to-end RAG (Retrieval-Augmented Generation) pipeline
- Fully local LLM inference (no external LLM APIs)
- Scalable document chunking and vector indexing
- Real-time semantic question answering
- Clean, reproducible Python environment design
- Production-oriented dependency and model management



ğŸ§  Architecture Overview

```

PDF â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ FAISS Index
â†“
Ollama LLM
â†“
Summary Generation & QA Responses

```

Key Design Decisions
- FAISS for fast vector similarity search
- Ollama embeddings for local semantic indexing
- LangChain chains for modular orchestration
- Streamlit for rapid, interactive UI development
- Virtual environments + pinned dependencies for reproducibility

---

âœ¨ Features

- ğŸ“„ Upload and parse PDF documents
- âœ‚ï¸ Chunk-based processing for large files
- ğŸ§  Vector embeddings using local models
- ğŸ” Semantic search with FAISS
- ğŸ“ Automatic document summarization
- â“ Context-aware question answering
- ğŸŒ Multilingual summary translation (AWS Translate)
- ğŸ’¾ Downloadable summaries

---

ğŸ§° Tech Stack

| Category | Tools |
|-------|------|
| Language | Python 3.11 |
| LLM Orchestration | LangChain (0.1.x) |
| Vector DB | FAISS |
| Local LLMs | Ollama |
| UI | Streamlit |
| PDF Parsing | pdfplumber |
| Translation | AWS Translate (boto3) |

---

ğŸ“¦ Requirements

System
- Python 3.11 (required)
- Ollama installed and running

=> Ollama Models:

```bash
ollama pull nomic-embed-text
ollama pull llama3
````

> Models are intentionally excluded from version control.

---

âš™ï¸ Setup & Installation

```bash
git clone https://github.com/<your-username>/literature-summarizer.git
cd literature-summarizer

python -m venv .venv
.venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

Run the application:

```bash
streamlit run litSummerisator.py
```

---

ğŸ§ª Usage

1. Upload a PDF document
2. Generate a concise summary
3. Ask semantic follow-up questions
4. Translate summaries if required
5. Download results

---

ğŸ” Engineering Practices

* Clean `.gitignore` (no models, venvs, or secrets)
* Pinned dependency versions
* Local inference for privacy & cost control
* Modular code structure
* Platform-agnostic setup

---

ğŸ§  What This Project Demonstrates

* Practical application of "LLMs" in real systems
* Building RAG pipelines from scratch
* Handling large documents efficiently
* Local AI deployment strategies
* Debugging complex Python environments
* Windows-specific ML tooling challenges and solutions

---

ğŸ”® Future Enhancements

* Streaming token responses
* Conversational memory improvements
* Dockerized deployment
* Migration to modern LangChain LCEL

---

ğŸ“œ License

This project is intended for educational and portfolio purposes.
Feel free to fork and extend.

---

ğŸ‘¤ Author
Ruthesh Medaboina
AI / ML Engineer | Python Developer

ğŸ“§ Email: [ruth.dev.v1@gmail.com](mailto:ruth.dev.v1@gmail.com)
ğŸ”— GitHub: [https://github.com/ruthesh-git]
ğŸ”— LinkedIn: [https://www.linkedin.com/in/ruthesh-medaboina-677b822a7/]

```
